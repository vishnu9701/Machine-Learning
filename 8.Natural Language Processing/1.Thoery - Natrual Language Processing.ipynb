{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJ0zpzSkySIz"
   },
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIJmD2ZbyhI-"
   },
   "source": [
    "Natural Language refers to the way we humans communicate with each other and processing is basically proceeding the data in an understandable form. so we can say that NLP (Natural Language Processing) is a way that helps computers to communicate with humans in their own language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48TH0KMBykOn"
   },
   "source": [
    "## NLTK: It is a python library that can we used to perform all the NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1676359631922,
     "user": {
      "displayName": "Vishnu Pandey",
      "userId": "14136930795597363772"
     },
     "user_tz": -330
    },
    "id": "9QWTiB5ZzFq3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\python\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python\\lib\\site-packages (from nltk) (2022.8.17)\n",
      "Requirement already satisfied: tqdm in c:\\python\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\python\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\python\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: colorama in c:\\python\\lib\\site-packages (from tqdm->nltk) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -treamlit (c:\\python\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\python\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\python\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\python\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\python\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\python\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    " pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJdqQV8zy92W"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80QaJ9Nay95o"
   },
   "source": [
    "Process of dividing the whole text into tokens.\n",
    "Words, numbers or punctuation marks can be tokens.\n",
    "It is mainly of two types:\n",
    "\n",
    "\n",
    "*   Word Tokenizer (separated by words)\n",
    "*   Sentence Tokenizer (separated by sentence)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyTGsQUz0sJY"
   },
   "source": [
    "# Word Tokenizer\n",
    "Divide the input text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 582,
     "status": "ok",
     "timestamp": 1676360015633,
     "user": {
      "displayName": "Vishnu Pandey",
      "userId": "14136930795597363772"
     },
     "user_tz": -330
    },
    "id": "t_QjISx-04dd",
    "outputId": "39421485-bc97-4aff-a4d1-988fea999956"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1676360258588,
     "user": {
      "displayName": "Vishnu Pandey",
      "userId": "14136930795597363772"
     },
     "user_tz": -330
    },
    "id": "93Z3lA2i1r72",
    "outputId": "4d1884e0-3133-4549-ce51-c396b7e01e45"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "text=\"My name is Vishnu Pandey, How are You?\"\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVCBG5Fhy9-7"
   },
   "source": [
    "## Sentence Tokenizer\n",
    "Divide the input text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1676360261132,
     "user": {
      "displayName": "Vishnu Pandey",
      "userId": "14136930795597363772"
     },
     "user_tz": -330
    },
    "id": "LlrH2mpG2nfc",
    "outputId": "ef1315ef-536d-4dab-f5bc-6e2c2ea942ce"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"My name is Vishnu Pandey\"\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNrerAhmy-C6"
   },
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVhbV52sy-FI"
   },
   "source": [
    "In general stopwords are the words in any language which does not add much meaning to a sentence. In NLP stopwords are those words which are not important in analyzing the data. <br>\n",
    "There are a total of 179 stopwords in English, using NLTK we can see all the stopwords in English.\n",
    "We Just need to import stopwords from the library nltk.corpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsOGCbyz38HD"
   },
   "source": [
    "### To remove Stopwords for a particular text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1676360827898,
     "user": {
      "displayName": "Vishnu Pandey",
      "userId": "14136930795597363772"
     },
     "user_tz": -330
    },
    "id": "7nhyNalx3ndk",
    "outputId": "c4d299ba-4d1a-455b-962d-de0f94e33d00"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "text=\"He is good boy,he is very good at coding \"\n",
    "text=word_tokenize(text)\n",
    "text_with_no_stopwords=[word for word in text if word not in stopwords.words('english')]\n",
    "text_with_no_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeYA5QIky-Hx"
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7Fl6R7zy-K7"
   },
   "source": [
    "Process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.\n",
    "\n",
    "\n",
    "*   loved → love, \n",
    "*    learning →learn <br>\n",
    "we can implement stemming by using PorterStemmer . we can import it from the library nltk.stem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1676361195358,
     "user": {
      "displayName": "Vishnu Pandey",
      "userId": "14136930795597363772"
     },
     "user_tz": -330
    },
    "id": "1WvTXzlM5erK",
    "outputId": "68ac48c8-0894-49fc-f5f4-a6dae51fe8e4"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "print(ps.stem(\"earning\"))\n",
    "print(ps.stem(\"greatful\"))\n",
    "print(ps.stem(\"studying\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdDobJSr5ePa"
   },
   "source": [
    "## Lemmatizing\n",
    "Lemmatization usually refers to doing things properly with the use of vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma <br>\n",
    "* lemmatization does the same work as stemming, the difference is that lemmatization returns a meaningful word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2274,
     "status": "ok",
     "timestamp": 1676361415699,
     "user": {
      "displayName": "Vishnu Pandey",
      "userId": "14136930795597363772"
     },
     "user_tz": -330
    },
    "id": "YGH6GMIn64kI",
    "outputId": "0171d08a-6e8f-402a-f152-e512b44264a5"
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 657,
     "status": "ok",
     "timestamp": 1676361586908,
     "user": {
      "displayName": "Vishnu Pandey",
      "userId": "14136930795597363772"
     },
     "user_tz": -330
    },
    "id": "u_a4VGYT7Wsp",
    "outputId": "f94ce5e8-19a9-4717-804f-da1137d1d5eb"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lem=WordNetLemmatizer()\n",
    "print(lem.lemmatize(\"earnings\"))\n",
    "print(lem.lemmatize(\"greatful\"))\n",
    "print(lem.lemmatize(\"studies\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6mX2DUXy-OK"
   },
   "source": [
    "## Bag of words\n",
    "Converting the data into vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1676361856956,
     "user": {
      "displayName": "Vishnu Pandey",
      "userId": "14136930795597363772"
     },
     "user_tz": -330
    },
    "id": "0z1VWyetyWYn",
    "outputId": "45421d6d-3d3d-4e7a-a4dd-73fcf408fa52"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sent1 = he is a good boy\n",
    "sent2 = she is a good girl\n",
    "sent3 = boy and girl are good \n",
    "        |\n",
    "        |\n",
    "  After removal of stopwords , lematization or stemming\n",
    "sent1 = good boy\n",
    "sent2 = good girl\n",
    "sent3 = boy girl good  \n",
    "        | ### Now we will calculate the frequency for each word by\n",
    "        |     calculating the occurrence of each word\n",
    "word  frequency\n",
    "good     3\n",
    "boy      2\n",
    "girl     2\n",
    "         | ## Then according to their occurrence we assign o or 1 \n",
    "         |    according to their occurrence in the sentence\n",
    "         | ## 1 for present and 0 fot not present\n",
    "         f1  f2   f3\n",
    "        girl good boy   \n",
    "sent1    0    1    1     \n",
    "sent2    1    0    1\n",
    "sent3    1    1    1\n",
    "### After this we pass the vector form to machine learning model\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnoWbLJB9GaY"
   },
   "source": [
    "### The above process can be done using a CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1676362026532,
     "user": {
      "displayName": "Vishnu Pandey",
      "userId": "14136930795597363772"
     },
     "user_tz": -330
    },
    "id": "4W1JoXQF8r3V",
    "outputId": "66d7c7e5-0996-4171-d634-0a736538f5d7"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "document = [\"One Geek helps Two Geeks\",\n",
    "\t\t\t\"Two Geeks help Four Geeks\",\n",
    "\t\t\t\"Each Geek helps many other Geeks at GeeksforGeeks\"]\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(document)\n",
    "print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
    "vector = vectorizer.transform(document)\n",
    "print(\"Encoded Document is:\")\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtH9Cxc5-R13"
   },
   "source": [
    "## TF-IDF (Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivVJndCn-TdG"
   },
   "source": [
    "**Term Frequency**: In document d, the frequency represents the number of instances of a given word t. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1676362251533,
     "user": {
      "displayName": "Vishnu Pandey",
      "userId": "14136930795597363772"
     },
     "user_tz": -330
    },
    "id": "fv3qOqY--dPB"
   },
   "outputs": [],
   "source": [
    "# tf(t,d) = count of t in d / number of words in d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hjvpEUU-ThI"
   },
   "source": [
    " **Document Frequency**: This tests the meaning of the text, which is very similar to TF, in the whole corpus collection. The only difference is that in document d, TF is the frequency counter for a term t, while df is the number of occurrences in the document set N of the term t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1676362294095,
     "user": {
      "displayName": "Vishnu Pandey",
      "userId": "14136930795597363772"
     },
     "user_tz": -330
    },
    "id": "raVntBIL-kFu"
   },
   "outputs": [],
   "source": [
    "# df(t) = occurrence of t in documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vput42-2-TjQ"
   },
   "source": [
    "**Inverse Document Frequency**: Mainly, it tests how relevant the word is. The key aim of the search is to locate the appropriate records that fit the demand. Since tf considers all terms equally significant, it is therefore not only possible to use the term frequencies to measure the weight of the term in the pap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1676362402038,
     "user": {
      "displayName": "Vishnu Pandey",
      "userId": "14136930795597363772"
     },
     "user_tz": -330
    },
    "id": "XeGTd-43-951"
   },
   "outputs": [],
   "source": [
    "# idf(t) = N/ df(t) = N/N(t)\n",
    "#idf(t) = log(N/ df(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1676362418894,
     "user": {
      "displayName": "Vishnu Pandey",
      "userId": "14136930795597363772"
     },
     "user_tz": -330
    },
    "id": "HbiPicp7_Gzu"
   },
   "outputs": [],
   "source": [
    "# tf-idf(t, d) = tf(t, d) * idf(t)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOwqr0UFtdsD0481WZ2swsG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
