{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6Yi67Zn8eY-"
   },
   "source": [
    "# Ensemble learning \n",
    "Ensemble learning combines multiple models to obtain better model performance. It helps you improve robustness and provide a generalized model. In short, it combines different decisions from the model to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_HIBWQJ8ggz"
   },
   "source": [
    "**Advantage :** Improvement in predictive accuracy.<br>\n",
    "**Disadvantage :** It is difficult to understand an ensemble of classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmM8JB2lqcQv"
   },
   "source": [
    "# Averaging\n",
    "We will take a simple average by adding all of the models' output and dividing it by the total number of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2NX0ZvfqcTj"
   },
   "source": [
    "## Weighted Average\n",
    " \n",
    "\n",
    "In the weighted average, we give the highest weight to best-performing models and the lowest weight to lower-performing models while taking an average. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bm0L5XWWqcVl"
   },
   "source": [
    "## Max Voting\n",
    " \n",
    "\n",
    "Max Voting is generally used for classification problems. In this method, each model makes a prediction and votes for each sample. From the sample class, only the highest-voted class is included in the final predictive class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "484j3s4FqcX-"
   },
   "source": [
    "## Stacking \n",
    " \n",
    "\n",
    "Stacking combines multiple base models via meta-model (meta-classifier or meta-regression). The base models are trained on a full dataset. The meta-model is trained on the features from base models(outputs). The base models and the meta-model are generally different. In short, meta-models help base models to find useful features to achieve high performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4tlaj1rqceh"
   },
   "source": [
    "## Bagging\n",
    " \n",
    "\n",
    "Bagging, also known as Bootstrap Aggregating, is an ensemble method to improve the stability and accuracy of machine learning models. It is used for minimizing variance and overfitting. Generally, it is applied to decision tree methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaPfFXJNqciR"
   },
   "source": [
    "## Boosting\n",
    " \n",
    "\n",
    "Boosting combines weak classifiers to create a strong classifier. It reduces biases and improves the single model performance. \n",
    "\n",
    "You can achieve high performance by training weak classifiers in series. The first model is training on training data, then the second model is trained to correct the error present in the first model. It is an iterative algorithm that adjusts weights based on the previous model. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNZkyzDA2+vJU01zz+9F0Gp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
